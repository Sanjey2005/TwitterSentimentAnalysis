{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - Data Preprocessing\n",
    "\n",
    "This notebook handles comprehensive data preprocessing for the Twitter sentiment analysis project.\n",
    "\n",
    "## Objectives\n",
    "1. Load and clean the dataset\n",
    "2. Handle Twitter-specific elements (hashtags, mentions, URLs, emojis)\n",
    "3. Implement text preprocessing (tokenization, lemmatization, stopword removal)\n",
    "4. Address class imbalance using SMOTE and weighted sampling\n",
    "5. Create feature extraction pipelines:\n",
    "   - TF-IDF vectorization\n",
    "   - Word2Vec embeddings\n",
    "   - GloVe embeddings\n",
    "6. Save processed data for model training\n",
    "\n",
    "## Preprocessing Pipeline\n",
    "1. **Text Cleaning**: Remove noise, normalize text\n",
    "2. **Twitter Elements**: Handle hashtags, mentions, URLs, emojis\n",
    "3. **Tokenization**: Split text into tokens\n",
    "4. **Normalization**: Lemmatization, stemming\n",
    "5. **Feature Extraction**: Create multiple feature representations\n",
    "6. **Class Balancing**: Address imbalanced dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Current working directory: c:\\Users\\acer\\Desktop\\ml_proj\\notebooks\n",
      "NLTK data downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Text processing libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Feature extraction libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Word embeddings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Set up\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    print(\"NLTK data downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"NLTK download issue: {e}\")\n",
    "\n",
    "# Small-run flag for laptops with limited RAM (16GB RAM, i5 11th gen CPU)\n",
    "# Using 20k tweets for fast training while maintaining good results\n",
    "SMALL_RUN = True\n",
    "SAMPLE_SIZE_TO_USE = 20000  # Optimized for laptop performance\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('../models/saved_models', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../reports/figures', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentiment140 dataset...\n",
      "Loading 20,000 tweets for preprocessing...\n",
      "Note: Sampling from both negative and positive sections to ensure balanced classes...\n",
      "Loading negative samples from first half...\n",
      "Loading positive samples from second half...\n",
      "Loading positive samples from second half...\n",
      "Dataset loaded: (20000, 6)\n",
      "\n",
      "Dataset shape: (20000, 8)\n",
      "Sentiment distribution:\n",
      "sentiment_label\n",
      "Positive    10000\n",
      "Negative    10000\n",
      "Name: count, dtype: int64\n",
      "Binary labels distribution: {1: 10000, 0: 10000}\n",
      "Memory usage: 7.57 MB\n",
      "Dataset loaded: (20000, 6)\n",
      "\n",
      "Dataset shape: (20000, 8)\n",
      "Sentiment distribution:\n",
      "sentiment_label\n",
      "Positive    10000\n",
      "Negative    10000\n",
      "Name: count, dtype: int64\n",
      "Binary labels distribution: {1: 10000, 0: 10000}\n",
      "Memory usage: 7.57 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading Sentiment140 dataset...\")\n",
    "\n",
    "# Define column names\n",
    "columns = ['sentiment', 'tweet_id', 'date', 'query', 'username', 'tweet_text']\n",
    "\n",
    "# Load dataset with a small sample for fast processing\n",
    "# IMPORTANT: The dataset is ordered (negatives first, then positives)\n",
    "# We need to sample from both sections to get balanced classes\n",
    "print(f\"Loading {SAMPLE_SIZE_TO_USE:,} tweets for preprocessing...\")\n",
    "print(\"Note: Sampling from both negative and positive sections to ensure balanced classes...\")\n",
    "\n",
    "# Strategy: Load samples from both halves of the dataset\n",
    "# Sentiment140 has ~800k negatives (sentiment=0) followed by ~800k positives (sentiment=4)\n",
    "half_sample = SAMPLE_SIZE_TO_USE // 2\n",
    "\n",
    "# Load from negative section (sample from first 400k rows to save memory)\n",
    "print(\"Loading negative samples from first half...\")\n",
    "df_neg_all = pd.read_csv('../sentiment140.csv', header=None, names=columns, encoding='latin-1', \n",
    "                         nrows=400000)\n",
    "df_neg = df_neg_all[df_neg_all['sentiment'] == 0].sample(n=half_sample, random_state=42).reset_index(drop=True)\n",
    "del df_neg_all\n",
    "\n",
    "# Load from positive section (sample from positive region, starting at row 800k)\n",
    "print(\"Loading positive samples from second half...\")\n",
    "df_pos_all = pd.read_csv('../sentiment140.csv', header=None, names=columns, encoding='latin-1', \n",
    "                         skiprows=800000, nrows=400000)\n",
    "df_pos = df_pos_all[df_pos_all['sentiment'] == 4].sample(n=half_sample, random_state=42).reset_index(drop=True)\n",
    "del df_pos_all\n",
    "\n",
    "# Combine both and shuffle\n",
    "df = pd.concat([df_neg, df_pos], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "del df_neg, df_pos  # Free memory\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "# Convert sentiment labels\n",
    "sentiment_mapping = {0: 'Negative', 4: 'Positive'}\n",
    "df['sentiment_label'] = df['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Create binary labels for modeling\n",
    "df['sentiment_binary'] = (df['sentiment'] == 4).astype(int)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Sentiment distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "print(f\"Binary labels distribution: {df['sentiment_binary'].value_counts().to_dict()}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessor initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing functions\n",
    "class TwitterTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing for Twitter data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.tweet_tokenizer = TweetTokenizer()\n",
    "        \n",
    "        # Twitter-specific patterns\n",
    "        self.url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        self.mention_pattern = re.compile(r'@\\\\w+')\n",
    "        self.hashtag_pattern = re.compile(r'#\\\\w+')\n",
    "        self.emoji_pattern = re.compile(r'[\\\\U0001F600-\\\\U0001F64F\\\\U0001F300-\\\\U0001F5FF\\\\U0001F680-\\\\U0001F6FF\\\\U0001F1E0-\\\\U0001F1FF]')\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = self.url_pattern.sub('URL', text)\n",
    "        \n",
    "        # Remove mentions\n",
    "        text = self.mention_pattern.sub('MENTION', text)\n",
    "        \n",
    "        # Handle hashtags (keep the word, remove #)\n",
    "        text = self.hashtag_pattern.sub(lambda m: m.group(0)[1:], text)\n",
    "        \n",
    "        # Remove emojis\n",
    "        text = self.emoji_pattern.sub('', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\\\s+', ' ', text)\n",
    "        \n",
    "        # Remove punctuation except for some important ones\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\\\s]', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Tokenize text using TweetTokenizer\"\"\"\n",
    "        return self.tweet_tokenizer.tokenize(text)\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords from tokens\"\"\"\n",
    "        return [token for token in tokens if token not in self.stop_words]\n",
    "    \n",
    "    def lemmatize_tokens(self, tokens):\n",
    "        \"\"\"Lemmatize tokens\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        \"\"\"Stem tokens\"\"\"\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    def preprocess_pipeline(self, text, use_lemmatization=True, use_stemming=False):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        # Clean text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenize_text(cleaned_text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        # Apply lemmatization or stemming\n",
    "        if use_lemmatization:\n",
    "            tokens = self.lemmatize_tokens(tokens)\n",
    "        elif use_stemming:\n",
    "            tokens = self.stem_tokens(tokens)\n",
    "        \n",
    "        # Filter out empty tokens\n",
    "        tokens = [token for token in tokens if len(token) > 1]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TwitterTextPreprocessor()\n",
    "print(\"Text preprocessor initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text preprocessing...\n",
      "Processing 20,000 tweets...\n",
      "Preprocessing tweets...\n",
      "Preprocessing completed!\n",
      "Tweets before: 20,000\n",
      "Tweets after: 19,990\n",
      "Removed empty tweets: 10\n",
      "\n",
      "Sample of preprocessed tweets:\n",
      "Original: @MitchBenn Take a photo, upload to Twitpic, ask Twitter to identify him. \n",
      "Cleaned:  mitchbenn take photo upload twitpic ask twitter identify\n",
      "--------------------------------------------------\n",
      "Original: Jacked up morning already. I'm in the emergency room. \n",
      "Cleaned:  jacked morning already emergency room\n",
      "--------------------------------------------------\n",
      "Original: I can't wait for summer to come! I  had snow on my car this morning \n",
      "Cleaned:  wait summer come snow car morning\n",
      "--------------------------------------------------\n",
      "Preprocessing completed!\n",
      "Tweets before: 20,000\n",
      "Tweets after: 19,990\n",
      "Removed empty tweets: 10\n",
      "\n",
      "Sample of preprocessed tweets:\n",
      "Original: @MitchBenn Take a photo, upload to Twitpic, ask Twitter to identify him. \n",
      "Cleaned:  mitchbenn take photo upload twitpic ask twitter identify\n",
      "--------------------------------------------------\n",
      "Original: Jacked up morning already. I'm in the emergency room. \n",
      "Cleaned:  jacked morning already emergency room\n",
      "--------------------------------------------------\n",
      "Original: I can't wait for summer to come! I  had snow on my car this morning \n",
      "Cleaned:  wait summer come snow car morning\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Apply text preprocessing\n",
    "print(\"Applying text preprocessing...\")\n",
    "print(f\"Processing {len(df):,} tweets...\")\n",
    "\n",
    "# Use the full dataset (already limited to SAMPLE_SIZE_TO_USE)\n",
    "df_sample = df.copy()\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing tweets...\")\n",
    "df_sample['cleaned_text'] = df_sample['tweet_text'].apply(\n",
    "    lambda x: preprocessor.preprocess_pipeline(x, use_lemmatization=True)\n",
    ")\n",
    "\n",
    "# Remove empty tweets after preprocessing\n",
    "initial_count = len(df_sample)\n",
    "df_sample = df_sample[df_sample['cleaned_text'].str.strip() != '']\n",
    "final_count = len(df_sample)\n",
    "\n",
    "print(f\"Preprocessing completed!\")\n",
    "print(f\"Tweets before: {initial_count:,}\")\n",
    "print(f\"Tweets after: {final_count:,}\")\n",
    "print(f\"Removed empty tweets: {initial_count - final_count:,}\")\n",
    "\n",
    "# Show sample of preprocessed text\n",
    "print(\"\\nSample of preprocessed tweets:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {df_sample.iloc[i]['tweet_text']}\")\n",
    "    print(f\"Cleaned:  {df_sample.iloc[i]['cleaned_text']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TF-IDF FEATURE EXTRACTION ===\n",
      "Fitting TF-IDF vectorizer...\n",
      "TF-IDF matrix shape: (19990, 5000)\n",
      "Vocabulary size: 5000\n",
      "Sample feature names: ['aaah' 'aaron' 'ab' 'abandoned' 'abc' 'ability' 'able' 'able make'\n",
      " 'able sleep' 'able talk']\n",
      "TF-IDF vectorizer saved to models/saved_models/tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction - TF-IDF\n",
    "print(\"=== TF-IDF FEATURE EXTRACTION ===\")\n",
    "\n",
    "# Initialize TF-IDF vectorizer (optimized for smaller dataset)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,   # Reduced vocabulary size for faster processing\n",
    "    ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "    min_df=3,            # Ignore terms that appear in less than 3 documents\n",
    "    max_df=0.95,         # Ignore terms that appear in more than 95% of documents\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_sample['cleaned_text'])\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Sample feature names: {feature_names[:10]}\")\n",
    "\n",
    "# Save TF-IDF vectorizer (ensure directory exists)\n",
    "os.makedirs(os.path.dirname('../models/saved_models/tfidf_vectorizer.pkl'), exist_ok=True)\n",
    "with open('../models/saved_models/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "print(\"TF-IDF vectorizer saved to models/saved_models/tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WORD2VEC EMBEDDINGS ===\n",
      "Preparing text data for Word2Vec...\n",
      "Training Word2Vec model...\n",
      "Word2Vec model trained!\n",
      "Vocabulary size: 5314\n",
      "Vector dimension: 100\n",
      "Word2Vec model saved to models/saved_models/word2vec_model.model\n",
      "Creating sentence embeddings...\n",
      "Word2Vec embeddings shape: (19990, 100)\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec embeddings\n",
    "print(\"=== WORD2VEC EMBEDDINGS ===\")\n",
    "\n",
    "# Prepare text data for Word2Vec (list of token lists)\n",
    "print(\"Preparing text data for Word2Vec...\")\n",
    "texts_for_w2v = [text.split() for text in df_sample['cleaned_text']]\n",
    "\n",
    "# Train Word2Vec model (optimized for faster training)\n",
    "print(\"Training Word2Vec model...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=texts_for_w2v,\n",
    "    vector_size=100,      # Embedding dimension\n",
    "    window=5,             # Context window size\n",
    "    min_count=3,          # Reduced minimum word frequency for smaller dataset\n",
    "    workers=2,            # Reduced workers for CPU efficiency\n",
    "    sg=0,                 # Use CBOW (0) or Skip-gram (1)\n",
    "    epochs=5              # Reduced epochs for faster training\n",
    ")\n",
    "\n",
    "print(f\"Word2Vec model trained!\")\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv)}\")\n",
    "print(f\"Vector dimension: {w2v_model.vector_size}\")\n",
    "\n",
    "# Save Word2Vec model\n",
    "w2v_model.save('../models/saved_models/word2vec_model.model')\n",
    "print(\"Word2Vec model saved to models/saved_models/word2vec_model.model\")\n",
    "\n",
    "# Create sentence embeddings by averaging word vectors\n",
    "def get_sentence_embedding(text, model):\n",
    "    \"\"\"Get sentence embedding by averaging word vectors\"\"\"\n",
    "    words = text.split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "print(\"Creating sentence embeddings...\")\n",
    "X_w2v = np.array([get_sentence_embedding(text, w2v_model) for text in df_sample['cleaned_text']])\n",
    "print(f\"Word2Vec embeddings shape: {X_w2v.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLASS IMBALANCE HANDLING ===\n",
      "Imbalance ratio: 1.00\n",
      "Dataset is well-balanced or SMOTE not applied. TF-IDF left unchanged.\n"
     ]
    }
   ],
   "source": [
    "# Class imbalance handling (do NOT apply SMOTE to sparse TF-IDF matrices — memory issue)\n",
    "print(\"=== CLASS IMBALANCE HANDLING ===\")\n",
    "\n",
    "y = df_sample['sentiment_binary']\n",
    "class_counts = y.value_counts()\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "if imbalance_ratio > 1.2:\n",
    "    print(\"\\nApplying SMOTE only to dense features (Word2Vec). TF-IDF kept as-is to avoid memory blowup.\")\n",
    "    # Ensure X_w2v is dense (it is built as dense above)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    try:\n",
    "        X_w2v_balanced, y_w2v_balanced = smote.fit_resample(X_w2v, y)\n",
    "        X_w2v = X_w2v_balanced\n",
    "        y = y_w2v_balanced\n",
    "        print(f\"Word2Vec shape after SMOTE: {X_w2v.shape}\")\n",
    "        print(f\"Balanced class distribution: {np.bincount(y)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"SMOTE on Word2Vec failed: {e}. Proceeding without SMOTE.\")\n",
    "else:\n",
    "    print(\"Dataset is well-balanced or SMOTE not applied. TF-IDF left unchanged.\")\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN-TEST SPLIT ===\n",
      "TF-IDF Train shape: (15992, 5000)\n",
      "TF-IDF Test shape: (3998, 5000)\n",
      "Word2Vec Train shape: (15992, 100)\n",
      "Word2Vec Test shape: (3998, 100)\n",
      "\\nTrain labels distribution: [7996 7996]\n",
      "Test labels distribution: [1999 1999]\n",
      "Converting sparse matrices to dense arrays...\n",
      "All processed data saved successfully!\n",
      "\\n✅ Preprocessing completed!\n",
      "Ready for model training in the next notebooks.\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "print(\"=== TRAIN-TEST SPLIT ===\")\n",
    "\n",
    "# Split the data\n",
    "X_tfidf_train, X_tfidf_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_w2v_train, X_w2v_test, y_w2v_train, y_w2v_test = train_test_split(\n",
    "    X_w2v, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"TF-IDF Train shape: {X_tfidf_train.shape}\")\n",
    "print(f\"TF-IDF Test shape: {X_tfidf_test.shape}\")\n",
    "print(f\"Word2Vec Train shape: {X_w2v_train.shape}\")\n",
    "print(f\"Word2Vec Test shape: {X_w2v_test.shape}\")\n",
    "\n",
    "print(f\"\\\\nTrain labels distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "# Save processed data (ensure directories exist)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Convert sparse matrices to dense arrays for saving (memory-efficient conversion)\n",
    "print(\"Converting sparse matrices to dense arrays...\")\n",
    "if hasattr(X_tfidf_train, 'toarray'):\n",
    "    X_tfidf_train_dense = X_tfidf_train.toarray()\n",
    "    X_tfidf_test_dense = X_tfidf_test.toarray()\n",
    "else:\n",
    "    X_tfidf_train_dense = X_tfidf_train\n",
    "    X_tfidf_test_dense = X_tfidf_test\n",
    "\n",
    "np.save('../data/processed/X_tfidf_train.npy', X_tfidf_train_dense)\n",
    "np.save('../data/processed/X_tfidf_test.npy', X_tfidf_test_dense)\n",
    "np.save('../data/processed/y_train.npy', y_train)\n",
    "np.save('../data/processed/y_test.npy', y_test)\n",
    "\n",
    "np.save('../data/processed/X_w2v_train.npy', X_w2v_train)\n",
    "np.save('../data/processed/X_w2v_test.npy', X_w2v_test)\n",
    "\n",
    "# Save text data for deep learning models\n",
    "text_train, text_test, _, _ = train_test_split(\n",
    "    df_sample['cleaned_text'], y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "with open('../data/processed/text_train.pkl', 'wb') as f:\n",
    "    pickle.dump(text_train.tolist(), f)\n",
    "\n",
    "with open('../data/processed/text_test.pkl', 'wb') as f:\n",
    "    pickle.dump(text_test.tolist(), f)\n",
    "\n",
    "print(\"All processed data saved successfully!\")\n",
    "print(\"\\\\n✅ Preprocessing completed!\")\n",
    "print(\"Ready for model training in the next notebooks.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
